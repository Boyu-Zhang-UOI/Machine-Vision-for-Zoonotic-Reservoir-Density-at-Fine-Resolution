{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.14.0.dev20221103+cu117\n",
      "Torchvision Version:  0.15.0.dev20221103+cu117\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from os.path import join\n",
    "from natsort import natsorted\n",
    "import rasterio\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "colors_list = ['yellow', 'pink', 'teal', 'gray', 'red', 'white', 'aqua', 'orange', 'darkblue']\n",
    "cmap = mpl.colors.ListedColormap(colors_list)\n",
    "bounds = [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 256]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 647 is out of bounds for axis 0 with size 432",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/ceph/boyuz/Builiding_Landscaping/land_conver_segmentation/partial_label/code/create_lc_features.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22416e65726f6964227d/mnt/ceph/boyuz/Builiding_Landscaping/land_conver_segmentation/partial_label/code/create_lc_features.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m imageio\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39m/mnt/ceph/onehealth/Boyu/land_cover/land_cover_features/bafodia_lc_fea_1000m_res_10m.tif\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22416e65726f6964227d/mnt/ceph/boyuz/Builiding_Landscaping/land_conver_segmentation/partial_label/code/create_lc_features.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22416e65726f6964227d/mnt/ceph/boyuz/Builiding_Landscaping/land_conver_segmentation/partial_label/code/create_lc_features.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39msum\u001b[39m(data[random\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m,\u001b[39m1000\u001b[39;49m), random\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m,\u001b[39m2000\u001b[39;49m), :])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 647 is out of bounds for axis 0 with size 432"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# data = imageio.imread('/mnt/ceph/onehealth/Boyu/land_cover/land_cover_features/bafodia_lc_fea_1000m_res_10m.tif')\n",
    "# print(data.dtype)\n",
    "# sum(data[random.randint(0,1000), random.randint(0,2000), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing Bafodia, radius 25\n"
     ]
    }
   ],
   "source": [
    "model_path = '/mnt/ceph/boyuz/Builiding_Landscaping/land_conver_segmentation/partial_label/code/cnn_model_5_epoch_30.pt'\n",
    "\n",
    "raster_list = [\n",
    "    '/mnt/ceph/boyuz/BU_Classifier/Bafodia_Google.tif'\n",
    "    ]\n",
    "\n",
    "pred_list = [\n",
    "    '/mnt/ceph/boyuz/Builiding_Landscaping/land_conver_segmentation/partial_label/code/64_5_Bafodia_Google.npy'\n",
    "]\n",
    "\n",
    "tile_size = 64\n",
    "step = 5 \n",
    "site_id = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PIXEL = 0.2204315\n",
    "\n",
    "radius = 25\n",
    "resolution = 1 if radius < 200 else 10\n",
    "factor = PIXEL/resolution\n",
    "site = raster_list[site_id].split(os.sep)[-1][:-4]\n",
    "print(f'Currently processing {site}, radius {str(radius)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original raster meta\n",
    "with rasterio.open(raster_list[site_id]) as src:\n",
    "    data = src.read()\n",
    "    profile = src.profile\n",
    "    transform = src.transform\n",
    "    height, width = src.height, src.width\n",
    "    crs = src.crs\n",
    "\n",
    "# create new raster_meta\n",
    "new_height, new_width = int(height * factor), int(width * factor)\n",
    "\n",
    "new_transform = transform * transform.scale(\n",
    "                    (width / new_width),\n",
    "                    (height / new_height)\n",
    "                )\n",
    "new_profile = profile\n",
    "\n",
    "new_profile.update(transform=new_transform, driver='GTiff', height=new_height, width=new_width, crs=crs, count=8)\n",
    "\n",
    "src.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction of CNN model\n",
    "predictions = np.load(pred_list[site_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6172, 7783)\n"
     ]
    }
   ],
   "source": [
    "# pad the edges with repeat num_lines times\n",
    "num_lines = len(list(range(0, 32, 5))) - 1\n",
    "\n",
    "first_row = predictions[0,:]\n",
    "repeated_first_row = np.repeat(np.expand_dims(first_row, 0), num_lines, axis=0)\n",
    "\n",
    "last_row = predictions[-1,:]\n",
    "repeated_last_row = np.repeat(np.expand_dims(last_row, 0), num_lines, axis=0)\n",
    "\n",
    "new_predictions = np.concatenate([repeated_first_row, predictions, repeated_last_row], axis=0)\n",
    "\n",
    "#expand vertically\n",
    "first_col = new_predictions[:, 0]\n",
    "repeated_first_col = np.repeat(np.expand_dims(first_col, 1), num_lines, axis=1)\n",
    "\n",
    "last_col = new_predictions[:, -1]\n",
    "repeated_last_col = np.repeat(np.expand_dims(last_col,1), num_lines, axis=1)\n",
    "\n",
    "new_predictions = np.concatenate([repeated_first_col, new_predictions, repeated_last_col], axis=1)\n",
    "print(new_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6172, 7783) (30864, 38915)\n"
     ]
    }
   ],
   "source": [
    "# resize the padded label to original raster size\n",
    "resized_preds = cv2.resize(new_predictions, (height, width), interpolation = cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40089\n"
     ]
    }
   ],
   "source": [
    "# create a cycle kernel based on distance\n",
    "k_size = int(round(radius/PIXEL))\n",
    "weights = np.zeros((k_size*2+1, k_size*2+1))\n",
    "num_kernal_pixel = 0\n",
    "for i in range(weights.shape[0]):\n",
    "    for j in range(weights.shape[1]):\n",
    "        if ((i-k_size)**2 + (j-k_size)**2)**0.5 <= k_size:\n",
    "            weights[i, j] = 1\n",
    "            num_kernal_pixel += 1\n",
    "print(num_kernal_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating category 0...\n"
     ]
    }
   ],
   "source": [
    "# calculate convolution for each class\n",
    "pixel_maps = []\n",
    "for class_idx in range(8):\n",
    "    print(f'Calculating category {str(class_idx)}...')\n",
    "    conv_target = copy.copy(resized_preds)\n",
    "    if class_idx == 0:\n",
    "        conv_target[conv_target==0] = 255\n",
    "        conv_target[conv_target!=255] = 0\n",
    "        conv_target[conv_target==255] = 1\n",
    "    else:\n",
    "        conv_target[conv_target!=class_idx] = 0\n",
    "        conv_target[conv_target==class_idx] = 1\n",
    "    \n",
    "    # calculate the building number with GPU\n",
    "    lbl_full_tensor = torch.from_numpy(conv_target).unsqueeze(0).unsqueeze(0)\n",
    "    # lbl_full_tensor = lbl_full_tensor\n",
    "    weights_tensor = torch.from_numpy(weights)\n",
    "    weights_tensor = weights_tensor.view(1, 1, weights_tensor.shape[0], weights_tensor.shape[1]).repeat(1, 1, 1, 1)\n",
    "    pixel_counts_idx = torch.nn.functional.conv2d(lbl_full_tensor.float().to(device), weights_tensor.float().to(device), padding='same')\n",
    "    pixel_counts_idx = np.squeeze(pixel_counts_idx.cpu().numpy()) #get back from gpu\n",
    "    pixel_counts_idx = pixel_counts_idx/num_kernal_pixel # get percentage\n",
    "\n",
    "    resized_pixel_counts= cv2.resize(pixel_counts_idx, (new_width, new_height))\n",
    "    pixel_maps.append(pixel_counts_idx)\n",
    "    break\n",
    "# save the results as a npy file\n",
    "np.save(f'lc_feature_{str(radius)}m_res_{str(resolution)}m', pixel_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# load the downsampled fc_feature\n",
    "pixel_maps = np.load(f'{site.lower()}_lc_fea_{str(radius)}m_res_{str(resolution)}m.npy')\n",
    "print(len(pixel_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new data according to the created profile\n",
    "with rasterio.open(f'{site.lower()}_lc_fea_{str(radius)}m_res_{str(resolution)}m.tif', 'w', **new_profile) as dst:\n",
    "    for i in range(len(pixel_maps)):\n",
    "        dst.write(pixel_maps[i].astype(float), i+1)\n",
    "dst.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('gis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57daff7361fc30c528f364da978aaf086b5665328c451b71e432dac8cbe449fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
